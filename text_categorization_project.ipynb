{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text categorization\n",
    "The goal of this program is to be able to categorize text into categories in the Brown corpus.\n",
    "Text categorization as a whole can be important and useful in many ways. Being able to easily categorize big amounts of documents can for example help with research. Automatic categorization can also make texts more easily available to people who are looking for certain types of texts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import some modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to plot graphics in your notebook, keep this %matplotlib command here before your imports\n",
    "%matplotlib notebook\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "import math    \n",
    "from nltk.corpus import brown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "Next define some functions that are needed for the program to work. These functions find certain features that exist in the data set and make a document more likely to belong to a certain category.\n",
    "\n",
    "The next function finds which words a certain document contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_features(document,word_features):\n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    label = \"contains\"\n",
    "    for word in word_features:\n",
    "        features['{:s}({:s})'.format(label,word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function finds the thirty first words of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_words(document,word_features):\n",
    "    document = document[:30]\n",
    "    document_words = set(document)\n",
    "    first_words = {}\n",
    "    label = \"begins\"\n",
    "    for word in word_features:\n",
    "        first_words['{:s}({:s})'.format(label,word)] = (word in document_words)\n",
    "    return first_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function finds the thirty last words of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_words(document,word_features):\n",
    "    document = document[-30:]\n",
    "    document_words = set(document)\n",
    "    last_words = {}\n",
    "    label = \"ends\"\n",
    "    for word in word_features:\n",
    "        last_words['{:s}({:s})'.format(label,word)] = (word in document_words)\n",
    "    return last_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function finds which bigrams the document contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_features(document,bigrams_most_common):\n",
    "    document = [\"{:s} {:s}\".format(w1, w2) for w1, w2 in nltk.bigrams(document)]\n",
    "    bigrams = {}\n",
    "    label = \"bigram\"\n",
    "    for bigram in bigrams_most_common:\n",
    "        bigrams['{:s}({:s})'.format(label,bigram)] = (bigram in document)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function finds trigrams in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigram_features(document,trigrams_most_common):\n",
    "    document = [\"{:s} {:s} {:s}\".format(w1, w2, w3) for w1, w2, w3 in nltk.trigrams(document)]\n",
    "    trigrams = {}\n",
    "    label = \"trigram\"\n",
    "    for trigram in trigrams_most_common:\n",
    "        trigrams['{:s}({:s})'.format(label,trigram)] = (trigram in document)\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function combines all the other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features_combined(document,word_features,bigrams_most_common,trigrams_most_common):\n",
    "    return {**contains_features(document, word_features),\n",
    "            **first_words(document, word_features),\n",
    "            **last_words(document, word_features),\n",
    "            **get_bigram_features(document,bigrams_most_common),\n",
    "            **get_trigram_features(document,trigrams_most_common)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "In the next cell we define the used data set and split it into a training set, a validation set and a test set. The training and validation sets are random and change every time the cell is run. These two sets are random because the used documents are shuffled in the beginning after the test set has been extracted from the whole data set. The documents are split into the training and the validation sets at the end of the next cell after the documents have been shuffled and all the document features have been found. The training set is 90% of the whole data and the test and validation sets are both 5% of all the documents. \n",
    "\n",
    "The test set is every 20th document from the end of the whole list of documents. This is because the documents in the list \"documents\" are ordered by category. By starting from the end of the list we are able to get all, even the smaller categories like science fiction, into the test set. If took every 20th document from the beginning the test set would not contain any science fiction texts.\n",
    "\n",
    "If you want to train the classifier again you have to run the next cell before running the cell where the training happens. The cell is slowish to run because the amount of both bigrams and trigrams is 3000 to make the classifier perform a little better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = [(list(brown.words(fileid)), category)\n",
    "            for category in brown.categories()\n",
    "            for fileid in brown.fileids(category)]\n",
    "\n",
    "#Here we get every 20th document of all the documents and make them the test set.\n",
    "test_set = documents[::-20]\n",
    "\n",
    "#Here we remove every item that is in the test set from the list \"documents\" so that the test set documents don't \n",
    "#appear in the training or validation sets.\n",
    "for item in documents:\n",
    "    if item in test_set:\n",
    "        documents.remove(item)\n",
    "\n",
    "#Here we shuffle the documents to make the training and validation sets random.\n",
    "random.shuffle(documents)\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "#We use only the 3000 most common words so that the program doesn't get too slow\n",
    "word_features = [w for w, f in all_words.most_common(3000)] \n",
    "    \n",
    "all_bigrams = nltk.FreqDist((w1.lower(), w2.lower()) for w1, w2 in nltk.bigrams(brown.words()))\n",
    "all_trigrams = nltk.FreqDist((w1.lower(), w2.lower(), w3.lower()) for w1, w2, w3 in nltk.trigrams(brown.words()))\n",
    "\n",
    "#Here we get the 3000 most commmon bigrams and trigrams out of all the bigrams in the corpus\n",
    "bigrams_most_common = [\"{:s} {:s}\".format(w1, w2) for ((w1, w2), f) in all_bigrams.most_common(3000)]\n",
    "trigrams_most_common = [\"{:s} {:s} {:s}\".format(w1, w2, w3) for ((w1, w2, w3), f) in all_trigrams.most_common(3000)]\n",
    "\n",
    "#Now we get all the features for all the documents\n",
    "featuresets = [(get_all_features_combined(d,word_features,bigrams_most_common,trigrams_most_common), c) for (d,c) in documents]\n",
    "test_set = [(get_all_features_combined(d,word_features,bigrams_most_common,trigrams_most_common), c) for (d,c) in test_set]\n",
    "\n",
    "#Here we split the featuresets into a train set and a test set\n",
    "train_set, validation_set = featuresets[25:], featuresets[:25]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers\n",
    "We use two algorithms to classify the texts: Naive Bayes and decision tree. This way we can compare the results and the classifiers' perfomances.\n",
    "\n",
    "### Naive Bayes\n",
    "A Naive Bayes classifier is a supervised machine learning algorithm, i.e. it is given the correct classifications for the train and test set. When the program is trained on the training set it finds features that make the text more likely to be in a certain category. Then when the program is run on the test set it looks which features the documents in the test set contain and calculates the propability of a document belonging to a certain category.\n",
    "\n",
    "The Naive Bayes algorithm assumes independence of the features in the data. That means that it assumes that the different features are not dependent on each other and that a certain feature doesn't make another feature more or less likely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classifier is trained on the train set\n",
    "naive_bayes_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree\n",
    "Decision tree is an algorithm that can be visualized as a flowchart. \"Decision nodes\" are nodes that check whether the document contains a certain feature. After going through several \"decision nodes\" the algorithm gets to a \"leaf node\" that assigns the category of the document. Decision tree is also a supervised machine learning.\n",
    "\n",
    "The next cell can be quite slow to run because a decision tree classifier is slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The classifier is trained on the train set\n",
    "decision_tree_classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation during testing\n",
    "The accuracies of the classifiers are evaluated here by using an already existing command `nltk.classify.accuracy()`. When the command is run on it first categorizes the documents in the given data set and then compares the categorization to the documents' actual categories that can be found in the data set. The evaluation should be run on the validation set because otherwise the classifiers accuracy will seem higher than it actually is. The validation set is a randomized set that is used for testing while developing the program. The actual test set is not random and it is used after the results on the validation set are good enough. This helps us avoid overfitting.\n",
    "\n",
    "### Majority baseline\n",
    "Because the most common category is \"learned\", the majority baseline is how accurate a classifier that classifies all documents as \"learned\" is. The whole data set consists of 500 documents of which 80 are in the \"learned\" category. So if we categorized all documents as \"learned\" 80 of them would be correct. 80 is 16% of 500 so it would mean that the majority baseline is 16%. To be useful at all the classifiers' accuracies should be more than 16%. \n",
    "\n",
    "The majority baseline is 16% also for the test set, because it contains 25 documents, 4 of which are in the category \"learned\". If we label all documents in the set as \"learner\", 4 documents will be classified correctly and 4 is 16% of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories in the whole data:\n",
      "Counter({'learned': 80, 'belles_lettres': 75, 'lore': 48, 'news': 44, 'hobbies': 36, 'government': 30, 'adventure': 29, 'fiction': 29, 'romance': 29, 'editorial': 27, 'mystery': 24, 'religion': 17, 'reviews': 17, 'humor': 9, 'science_fiction': 6})\n",
      "Categories in the test set:\n",
      "Counter({'learned': 4, 'belles_lettres': 4, 'lore': 3, 'news': 2, 'hobbies': 2, 'fiction': 2, 'science_fiction': 1, 'romance': 1, 'reviews': 1, 'religion': 1, 'mystery': 1, 'government': 1, 'editorial': 1, 'adventure': 1})\n"
     ]
    }
   ],
   "source": [
    "#Here we get all the categories and how many documents each category contains in both the whole data set and\n",
    "#only the test set.\n",
    "print(\"Categories in the whole data:\")\n",
    "categories = list(category for category in brown.categories() for fileid in brown.fileids(category))\n",
    "print(Counter(categories))\n",
    "print(\"Categories in the test set:\")\n",
    "categories = list(category for category in brown.categories() for fileid in brown.fileids(category))[::-20]\n",
    "print(Counter(categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell we evaluate the classifiers on the validation set. This is not the final result and not the \"actual\" accuracy of the classifiers. The accuracy that really matters is the one that we get on the test set. We will get that accuracy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Naive Bayes classifier: \n",
      "0.56\n",
      "Accuracy of the decision tree classifier\n",
      "0.24\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of the Naive Bayes classifier: ')\n",
    "#We get the accuracy of the classifier by running it on the validation set\n",
    "print(nltk.classify.accuracy(naive_bayes_classifier, validation_set))\n",
    "print('Accuracy of the decision tree classifier')\n",
    "print(nltk.classify.accuracy(decision_tree_classifier, validation_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most informative features\n",
    "In the next cell we print 30 features that the classifier has calculated to have the biggest effect on which category a document belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        contains(didn't) = True           myster : learne =     45.5 : 1.0\n",
      "            bigram(me ,) = True            humor : learne =     35.1 : 1.0\n",
      "          contains(dark) = True           advent : learne =     33.9 : 1.0\n",
      "        contains(season) = True           review : learne =     32.9 : 1.0\n",
      "        contains(wasn't) = True           myster : learne =     32.8 : 1.0\n",
      "      contains(wouldn't) = True           myster : learne =     32.8 : 1.0\n",
      "          bigram(you '') = True           romanc : learne =     30.9 : 1.0\n",
      "        bigram(his wife) = True            humor : learne =     29.7 : 1.0\n",
      "         bigram(and she) = True           romanc : learne =     29.0 : 1.0\n",
      "           contains(ran) = True           advent : learne =     28.7 : 1.0\n",
      "        contains(walked) = True           advent : learne =     28.7 : 1.0\n",
      "         bigram(asked .) = True           scienc : learne =     28.4 : 1.0\n",
      "        bigram(his mind) = True           scienc : learne =     28.4 : 1.0\n",
      "       bigram(the night) = True           scienc : learne =     28.4 : 1.0\n",
      "        bigram(the ship) = True           scienc : learne =     28.4 : 1.0\n",
      "         bigram(you are) = True           scienc : learne =     28.4 : 1.0\n",
      "          contains(tiny) = True           scienc : learne =     28.4 : 1.0\n",
      "       contains(kitchen) = True            humor : belles =     27.7 : 1.0\n",
      "         contains(shoes) = True            humor : belles =     27.7 : 1.0\n",
      "       bigram(the music) = True           review : learne =     27.2 : 1.0\n",
      "       contains(playing) = True           review : learne =     27.2 : 1.0\n",
      "       bigram(she could) = True           romanc : learne =     27.1 : 1.0\n",
      "      contains(watching) = True           romanc : learne =     27.1 : 1.0\n",
      "        contains(caught) = True           advent : learne =     26.9 : 1.0\n",
      "        trigram('' . ``) = True           scienc : learne =     26.8 : 1.0\n",
      "       bigram(was going) = True           myster : learne =     26.4 : 1.0\n",
      "       contains(waiting) = True           myster : learne =     26.4 : 1.0\n",
      "         bigram(if this) = True           scienc : belles =     26.4 : 1.0\n",
      "          bigram(, said) = True             news : learne =     25.6 : 1.0\n",
      "       contains(watched) = True           advent : learne =     25.2 : 1.0\n",
      "            bigram(, my) = True            humor : learne =     24.3 : 1.0\n",
      "           bigram(a lot) = True            humor : learne =     24.3 : 1.0\n",
      "          bigram(back .) = True           myster : learne =     24.3 : 1.0\n",
      "         bigram(house ,) = True            humor : learne =     24.3 : 1.0\n",
      "          bigram(that .) = True           myster : learne =     24.3 : 1.0\n",
      "         contains(throw) = True            humor : learne =     24.3 : 1.0\n",
      "    trigram(, this time) = True            humor : learne =     24.3 : 1.0\n",
      "   trigram(was going to) = True           myster : learne =     24.3 : 1.0\n",
      "        contains(smiled) = True           romanc : belles =     23.5 : 1.0\n",
      "       trigram(him . ``) = True           advent : learne =     23.5 : 1.0\n",
      "        bigram(his face) = True           fictio : learne =     23.4 : 1.0\n",
      "     contains(afternoon) = True           fictio : learne =     23.4 : 1.0\n",
      "       contains(clothes) = True           romanc : learne =     23.4 : 1.0\n",
      "         contains(minds) = True           religi : learne =     22.8 : 1.0\n",
      "         contains(teeth) = True            humor : belles =     22.7 : 1.0\n",
      "     contains(telephone) = True            humor : belles =     22.7 : 1.0\n",
      "          bigram(her to) = True           myster : learne =     22.2 : 1.0\n",
      "           contains(yes) = True           myster : learne =     22.2 : 1.0\n",
      "        contains(you're) = True           myster : learne =     22.2 : 1.0\n",
      "         bigram(had had) = True           scienc : learne =     21.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#We print 50 most informative features in the data.\n",
    "naive_bayes_classifier.show_most_informative_features(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, most of the most informative are the bigrams, trigrams and words that a document contains. The other features do not show up among the most informative features that often.\n",
    "\n",
    "Let's see what the list of most informative features means. Let's imagine we have the following row in the list:\n",
    "\n",
    "contains(didn't) = True    myster:learne =   44.9  :  1.0\n",
    "\n",
    "The first \"column\" tells us which feature we are talking about, in this case it's the words a document contains and the word is \"didn't\". The \"True\" after the equal sign tells us that the document should contain this feature. If the word after the equal sign was \"False\" it would mean that the feature should not be in the document. The next column shows which two categories we are comparing and the numbers after the categories tell the likelihood of the cateogries when the document contains the feature. In this case this means that if the document contains the word \"didn't\" it is 44.9% more likely to be in the category \"mystery\" than in the \"learned\" category.\n",
    "\n",
    "It seems that the second category of the two compared categories is always either \"learned\" or \"belles letres\". These are the two biggest categories in the data set. That could mean that because they are so common it is easier for the classifier to find features that would not make a document belong to these categories than to find features that make a document to be \"learned\" or \"belles letres\".\n",
    "\n",
    "\n",
    "### Pseudocode of the decision tree classifier\n",
    "\n",
    "There is no way to get the most informative features for the decision tree classifier as there is for the Naive Bayes classifier. The way to see the rules by which the decision tree classifier categorizes the documents is to print the pseudocode for the classifier. You can change the depth, i.e. how many of \"rules\" the classifier has created you want to see.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if bigram(, he) == False: \n",
      "  if contains(you) == False: \n",
      "    if contains(1961) == False: \n",
      "      if contains(approval) == False: \n",
      "        if contains(woman) == False: \n",
      "          if contains(proud) == False: return 'learned'\n",
      "          if contains(proud) == True: return 'belles_lettres'\n",
      "        if contains(woman) == True: \n",
      "          if contains(3) == False: return 'lore'\n",
      "          if contains(3) == True: return 'reviews'\n",
      "      if contains(approval) == True: \n",
      "        if contains(3) == False: return 'lore'\n",
      "        if contains(3) == True: return 'government'\n",
      "    if contains(1961) == True: \n",
      "      if contains(committee) == False: \n",
      "        if contains(show) == False: \n",
      "          if contains(nation's) == False: return 'government'\n",
      "          if contains(nation's) == True: return 'belles_lettres'\n",
      "        if contains(show) == True: \n",
      "          if contains(nation's) == False: return 'learned'\n",
      "          if contains(nation's) == True: return 'editorial'\n",
      "      if contains(committee) == True: \n",
      "        if bigram(`` for) == False: return 'news'\n",
      "        if bigram(`` for) == True: return 'lore'\n",
      "  if contains(you) == True: \n",
      "    if bigram(concerning the) == False: \n",
      "      if bigram(knowledge of) == False: \n",
      "        if contains(generation) == False: \n",
      "          if trigram(said , ``) == False: return 'fiction'\n",
      "          if trigram(said , ``) == True: return 'romance'\n",
      "        if contains(generation) == True: \n",
      "          if trigram(the future of) == False: return 'editorial'\n",
      "          if trigram(the future of) == True: return 'learned'\n",
      "      if bigram(knowledge of) == True: \n",
      "        if trigram(the future of) == False: return 'belles_lettres'\n",
      "        if trigram(the future of) == True: return 'religion'\n",
      "    if bigram(concerning the) == True: return 'learned'\n",
      "if bigram(, he) == True: \n",
      "  if contains(week) == False: \n",
      "    if bigram(you '') == False: \n",
      "      if contains(heard) == False: \n",
      "        if contains(selection) == False: \n",
      "          if contains(3) == False: return 'government'\n",
      "          if contains(3) == True: return 'editorial'\n",
      "        if contains(selection) == True: \n",
      "          if trigram(it was a) == False: return 'learned'\n",
      "          if trigram(it was a) == True: return 'reviews'\n",
      "      if contains(heard) == True: \n",
      "        if bigram(have to) == False: \n",
      "          if bigram(happened to) == False: return 'learned'\n",
      "          if bigram(happened to) == True: return 'belles_lettres'\n",
      "        if bigram(have to) == True: \n",
      "          if bigram(made of) == False: return 'editorial'\n",
      "          if bigram(made of) == True: return 'mystery'\n",
      "    if bigram(you '') == True: \n",
      "      if contains(because) == False: \n",
      "        if bigram(was one) == False: \n",
      "          if bigram(has been) == False: return 'mystery'\n",
      "          if bigram(has been) == True: return 'lore'\n",
      "        if bigram(was one) == True: return 'fiction'\n",
      "      if contains(because) == True: \n",
      "        if bigram(out the) == False: \n",
      "          if bigram(center of) == False: return 'romance'\n",
      "          if bigram(center of) == True: return 'science_fiction'\n",
      "        if bigram(out the) == True: \n",
      "          if trigram(and he could) == False: return 'mystery'\n",
      "          if trigram(and he could) == True: return 'adventure'\n",
      "  if contains(week) == True: \n",
      "    if contains(wasn't) == False: \n",
      "      if contains(wrote) == False: \n",
      "        if bigram(him with) == False: \n",
      "          if bigram(is too) == False: return 'news'\n",
      "          if bigram(is too) == True: return 'editorial'\n",
      "        if bigram(him with) == True: \n",
      "          if contains(3) == False: return 'fiction'\n",
      "          if contains(3) == True: return 'reviews'\n",
      "      if contains(wrote) == True: \n",
      "        if bigram(the old) == False: \n",
      "          if trigram(, and the) == False: return 'news'\n",
      "          if trigram(, and the) == True: return 'belles_lettres'\n",
      "        if bigram(the old) == True: \n",
      "          if trigram(on the road) == False: return 'editorial'\n",
      "          if trigram(on the road) == True: return 'hobbies'\n",
      "    if contains(wasn't) == True: \n",
      "      if trigram(to see the) == False: \n",
      "        if trigram(now ? ?) == False: \n",
      "          if contains(pleased) == False: return 'romance'\n",
      "          if contains(pleased) == True: return 'science_fiction'\n",
      "        if trigram(now ? ?) == True: return 'fiction'\n",
      "      if trigram(to see the) == True: return 'mystery'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decision_tree_classifier.pseudocode(depth=6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudocode is not as easy to read as the most informative features of the Naive Bayes classifier, but it can still be quite informative. For example, if we have the following pseudocode:\n",
    "\n",
    "if bigram(, he) == False: \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;  if contains(you) == False:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    if contains(projects) == False:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;      if bigram(when it) == False:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;        if bigram(a part) == False: \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          if contains(sign) == False: return 'learned'\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          if contains(sign) == True: return 'news'\n",
    "          \n",
    "Here we see that if the document doesn't contain the bigram \", he\" and it doesn't contain the word \"you\" and it doesn't contain that word \"projects\" and it doesn't contain the bigram \"when it\" and it doesn't contain the bigram \"a part\" and it doesn't contain the word \"sign\", the document is classified as \"learned. If the document doesn't contain all the other words but does contain the word \"sign\", it is classified as \"news\".\n",
    "\n",
    "Even though the pseudocode does show us the rules the classifier uses classifies the documents, they don't make a lot of sense. It is very hard to know why the bigram \", he\" would make a document less likely to belong to a certain category.\n",
    "\n",
    "## Final evaluation and categorization examples\n",
    "\n",
    "Here we see the actual accuracy when the program is run on the test set that is always the same and does not depend on the run. We also see how the classifiers classify the documents in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual accuracy of the Naive Bayes classifier: \n",
      "0.64\n",
      "\n",
      "Actual accuracy of the decision tree classifier: \n",
      "0.28\n",
      "\n",
      "['It', 'would', 'have', 'killed', 'you', 'in', 'the', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "fiction\n",
      "Category given by the decision tree classifier:\n",
      "religion\n",
      "Real category:\n",
      "['science_fiction']\n",
      "\n",
      "['``', 'They', 'make', 'us', 'conformists', 'look', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "romance\n",
      "Category given by the decision tree classifier:\n",
      "fiction\n",
      "Real category:\n",
      "['romance']\n",
      "\n",
      "['Radio', 'is', 'easily', 'outdistancing', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "belles_lettres\n",
      "Category given by the decision tree classifier:\n",
      "religion\n",
      "Real category:\n",
      "['reviews']\n",
      "\n",
      "['Few', 'persons', 'who', 'join', 'the', 'Church', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "learned\n",
      "Category given by the decision tree classifier:\n",
      "news\n",
      "Real category:\n",
      "['religion']\n",
      "\n",
      "['At', 'last', 'the', 'White', 'House', 'is', 'going', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "news\n",
      "Category given by the decision tree classifier:\n",
      "belles_lettres\n",
      "Real category:\n",
      "['news']\n",
      "\n",
      "['Rookie', 'Ron', 'Nischwitz', 'continued', 'his', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "news\n",
      "Category given by the decision tree classifier:\n",
      "lore\n",
      "Real category:\n",
      "['news']\n",
      "\n",
      "['The', 'safe', 'at', 'Ingleside', 'District', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "mystery\n",
      "Category given by the decision tree classifier:\n",
      "adventure\n",
      "Real category:\n",
      "['mystery']\n",
      "\n",
      "['With', 'capital', 'largely', 'squandered', ',', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "belles_lettres\n",
      "Category given by the decision tree classifier:\n",
      "belles_lettres\n",
      "Real category:\n",
      "['lore']\n",
      "\n",
      "['My', 'interviews', 'with', 'teen-agers', 'confirmed', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "lore\n",
      "Category given by the decision tree classifier:\n",
      "editorial\n",
      "Real category:\n",
      "['lore']\n",
      "\n",
      "['Buffeted', 'by', 'swirling', 'winds', ',', 'the', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "adventure\n",
      "Category given by the decision tree classifier:\n",
      "romance\n",
      "Real category:\n",
      "['lore']\n",
      "\n",
      "['So', 'far', 'these', 'remarks', ',', 'like', 'most', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "belles_lettres\n",
      "Category given by the decision tree classifier:\n",
      "belles_lettres\n",
      "Real category:\n",
      "['learned']\n",
      "\n",
      "['A', '.', 'Reasons', 'for', 'selecting', 'mail', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "learned\n",
      "Category given by the decision tree classifier:\n",
      "government\n",
      "Real category:\n",
      "['learned']\n",
      "\n",
      "['Sentiment', '.', 'Tension', 'management', 'and', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "learned\n",
      "Category given by the decision tree classifier:\n",
      "learned\n",
      "Real category:\n",
      "['learned']\n",
      "\n",
      "['Polyphosphates', 'gave', 'renewed', 'life', 'to', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "learned\n",
      "Category given by the decision tree classifier:\n",
      "government\n",
      "Real category:\n",
      "['learned']\n",
      "\n",
      "['General', 'How', 'long', 'has', 'it', 'been', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "hobbies\n",
      "Category given by the decision tree classifier:\n",
      "hobbies\n",
      "Real category:\n",
      "['hobbies']\n",
      "\n",
      "['The', 'average', 'reader', 'of', 'this', 'magazine', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "hobbies\n",
      "Category given by the decision tree classifier:\n",
      "hobbies\n",
      "Real category:\n",
      "['hobbies']\n",
      "\n",
      "['Wildlife', 'habitat', 'resources', 'In', '1960', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "government\n",
      "Category given by the decision tree classifier:\n",
      "learned\n",
      "Real category:\n",
      "['government']\n",
      "\n",
      "['Was', 'it', 'love', '?', '?', 'I', 'had', 'no', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "fiction\n",
      "Category given by the decision tree classifier:\n",
      "belles_lettres\n",
      "Real category:\n",
      "['fiction']\n",
      "\n",
      "['It', 'was', 'the', 'first', 'time', 'any', 'of', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "adventure\n",
      "Category given by the decision tree classifier:\n",
      "lore\n",
      "Real category:\n",
      "['fiction']\n",
      "\n",
      "['``', 'A', 'lousy', 'job', \"''\", 'Chicago', ',', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "news\n",
      "Category given by the decision tree classifier:\n",
      "editorial\n",
      "Real category:\n",
      "['editorial']\n",
      "\n",
      "['For', 'here', 'if', 'anywhere', 'in', 'contemporary', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "belles_lettres\n",
      "Category given by the decision tree classifier:\n",
      "belles_lettres\n",
      "Real category:\n",
      "['belles_lettres']\n",
      "\n",
      "['Mando', ',', 'pleading', 'her', 'cause', ',', 'must', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "lore\n",
      "Category given by the decision tree classifier:\n",
      "fiction\n",
      "Real category:\n",
      "['belles_lettres']\n",
      "\n",
      "[\"Henrietta's\", 'feeling', 'of', 'identity', 'with', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "belles_lettres\n",
      "Category given by the decision tree classifier:\n",
      "belles_lettres\n",
      "Real category:\n",
      "['belles_lettres']\n",
      "\n",
      "['As', 'cells', 'coalesced', 'into', 'organisms', ',', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "belles_lettres\n",
      "Category given by the decision tree classifier:\n",
      "religion\n",
      "Real category:\n",
      "['belles_lettres']\n",
      "\n",
      "['They', 'were', 'west', 'of', 'the', 'Sabine', ',', ...]\n",
      "Category given by the Naive Bayes classifier:\n",
      "adventure\n",
      "Category given by the decision tree classifier:\n",
      "adventure\n",
      "Real category:\n",
      "['adventure']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Actual accuracy of the Naive Bayes classifier: ')\n",
    "#We get the real accuracy of the classifier by running it on the test set\n",
    "print(nltk.classify.accuracy(naive_bayes_classifier, test_set))\n",
    "print()\n",
    "\n",
    "print('Actual accuracy of the decision tree classifier: ')\n",
    "#We get the real accuracy of the classifier by running it on the test set\n",
    "print(nltk.classify.accuracy(decision_tree_classifier, test_set))\n",
    "print()\n",
    "\n",
    "#Here we get the fileids in the same order as when we define the list \"documents\". This way we can get the file ids \n",
    "#for the documents in the test set.\n",
    "fileids = [fileid for category in brown.categories()\n",
    "            for fileid in brown.fileids(category)]\n",
    "fileids = fileids[::-20]\n",
    "\n",
    "#Now we print the beginnings of the documents in the test set, how they are categorized by the program and their real \n",
    "#categories.\n",
    "for id in fileids:\n",
    "    print(brown.words(id))\n",
    "    print(\"Category given by the Naive Bayes classifier:\")\n",
    "    print(naive_bayes_classifier.classify(get_all_features_combined(list(brown.words(id)), word_features, bigrams_most_common, trigrams_most_common)))\n",
    "    print(\"Category given by the decision tree classifier:\")\n",
    "    print(decision_tree_classifier.classify(get_all_features_combined(list(brown.words(id)), word_features, bigrams_most_common, trigrams_most_common)))\n",
    "    print(\"Real category:\")\n",
    "    print(brown.categories(id))\n",
    "    print()             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrixes\n",
    "\n",
    "Next we make a confusion matrixes for the classifiers. It is quite easy to compare how well the classifiers have categorized the documents in the test set. \n",
    "\n",
    "The rows in the matrix are the reference or the gold standard and the columns show the test performance. The numbers inside \"<>\" is the percentage of documents in the category classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                |                                                                                                 s |\n",
      "                |             b                                                                                   c |\n",
      "                |             e                                                                                   i |\n",
      "                |             l                                                                                   e |\n",
      "                |             l                                                                                   n |\n",
      "                |             e                    g                                                              c |\n",
      "                |      a      s      e             o                                                              e |\n",
      "                |      d      _      d             v                                         r                    _ |\n",
      "                |      v      l      i      f      e      h      l             m             e      r      r      f |\n",
      "                |      e      e      t      i      r      o      e             y             l      e      o      i |\n",
      "                |      n      t      o      c      n      b      a             s             i      v      m      c |\n",
      "                |      t      t      r      t      m      b      r      l      t      n      g      i      a      t |\n",
      "                |      u      r      i      i      e      i      n      o      e      e      i      e      n      i |\n",
      "                |      r      e      a      o      n      e      e      r      r      w      o      w      c      o |\n",
      "                |      e      s      l      n      t      s      d      e      y      s      n      s      e      n |\n",
      "----------------+---------------------------------------------------------------------------------------------------+\n",
      "      adventure |  <4.0%>     .      .      .      .      .      .      .      .      .      .      .      .      . |\n",
      " belles_lettres |      . <12.0%>     .      .      .      .      .   4.0%      .      .      .      .      .      . |\n",
      "      editorial |      .      .     <.>     .      .      .      .      .      .   4.0%      .      .      .      . |\n",
      "        fiction |   4.0%      .      .  <4.0%>     .      .      .      .      .      .      .      .      .      . |\n",
      "     government |      .      .      .      .  <4.0%>     .      .      .      .      .      .      .      .      . |\n",
      "        hobbies |      .      .      .      .      .  <8.0%>     .      .      .      .      .      .      .      . |\n",
      "        learned |      .   4.0%      .      .      .      . <12.0%>     .      .      .      .      .      .      . |\n",
      "           lore |   4.0%   4.0%      .      .      .      .      .  <4.0%>     .      .      .      .      .      . |\n",
      "        mystery |      .      .      .      .      .      .      .      .  <4.0%>     .      .      .      .      . |\n",
      "           news |      .      .      .      .      .      .      .      .      .  <8.0%>     .      .      .      . |\n",
      "       religion |      .      .      .      .      .      .   4.0%      .      .      .     <.>     .      .      . |\n",
      "        reviews |      .   4.0%      .      .      .      .      .      .      .      .      .     <.>     .      . |\n",
      "        romance |      .      .      .      .      .      .      .      .      .      .      .      .  <4.0%>     . |\n",
      "science_fiction |      .      .      .   4.0%      .      .      .      .      .      .      .      .      .     <.>|\n",
      "----------------+---------------------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gold is the real category of each document in the test set.\n",
    "gold = []\n",
    "for item in test_set:\n",
    "    gold.append(item[1])\n",
    "    \n",
    "#The confusion tree for the Naive Bayes classifier\n",
    "test = naive_bayes_classifier.classify_many(f for (f, c) in test_set)\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(cm.pretty_format(sort_by_count=False, show_percents=True, truncate=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                |                                                                                                 s |\n",
      "                |             b                                                                                   c |\n",
      "                |             e                                                                                   i |\n",
      "                |             l                                                                                   e |\n",
      "                |             l                                                                                   n |\n",
      "                |             e                    g                                                              c |\n",
      "                |      a      s      e             o                                                              e |\n",
      "                |      d      _      d             v                                         r                    _ |\n",
      "                |      v      l      i      f      e      h      l             m             e      r      r      f |\n",
      "                |      e      e      t      i      r      o      e             y             l      e      o      i |\n",
      "                |      n      t      o      c      n      b      a             s             i      v      m      c |\n",
      "                |      t      t      r      t      m      b      r      l      t      n      g      i      a      t |\n",
      "                |      u      r      i      i      e      i      n      o      e      e      i      e      n      i |\n",
      "                |      r      e      a      o      n      e      e      r      r      w      o      w      c      o |\n",
      "                |      e      s      l      n      t      s      d      e      y      s      n      s      e      n |\n",
      "----------------+---------------------------------------------------------------------------------------------------+\n",
      "      adventure |  <4.0%>     .      .      .      .      .      .      .      .      .      .      .      .      . |\n",
      " belles_lettres |      .  <8.0%>     .   4.0%      .      .      .      .      .      .   4.0%      .      .      . |\n",
      "      editorial |      .      .  <4.0%>     .      .      .      .      .      .      .      .      .      .      . |\n",
      "        fiction |      .   4.0%      .     <.>     .      .      .   4.0%      .      .      .      .      .      . |\n",
      "     government |      .      .      .      .     <.>     .   4.0%      .      .      .      .      .      .      . |\n",
      "        hobbies |      .      .      .      .      .  <8.0%>     .      .      .      .      .      .      .      . |\n",
      "        learned |      .   4.0%      .      .   8.0%      .  <4.0%>     .      .      .      .      .      .      . |\n",
      "           lore |      .   4.0%   4.0%      .      .      .      .     <.>     .      .      .      .   4.0%      . |\n",
      "        mystery |   4.0%      .      .      .      .      .      .      .     <.>     .      .      .      .      . |\n",
      "           news |      .   4.0%      .      .      .      .      .   4.0%      .     <.>     .      .      .      . |\n",
      "       religion |      .      .      .      .      .      .      .      .      .   4.0%     <.>     .      .      . |\n",
      "        reviews |      .      .      .      .      .      .      .      .      .      .   4.0%     <.>     .      . |\n",
      "        romance |      .      .      .   4.0%      .      .      .      .      .      .      .      .     <.>     . |\n",
      "science_fiction |      .      .      .      .      .      .      .      .      .      .   4.0%      .      .     <.>|\n",
      "----------------+---------------------------------------------------------------------------------------------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The confusion matrix for the decision tree classifier\n",
    "test = decision_tree_classifier.classify_many(f for (f, c) in test_set)\n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(cm.pretty_format(sort_by_count=False, show_percents=True, truncate=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final discussion\n",
    "\n",
    "Neither of the classifiers is not very accurate. The Naive Bayes classifier's performance varies a lot from run to run. Its performance varies from about 30% to around 60% which is not very good. The decision tree performs worse than the Naive Bayes classifier on every run.\n",
    "\n",
    "Compared to the majority baseline the Naive Bayes classifier performs always better than 16% so one could say that this classifier could be somewhat useful. The decision tree performs sometimes worse than what the majority baseline is, so it is not really a classifier that should be used at all.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
